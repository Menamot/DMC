{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salva\\AppData\\Roaming\\Python\\Python39\\site-packages\\matplotlib\\projections\\__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([13, 15, 13, ..., 23,  7, 17])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from DMC_class import * \n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "k=50\n",
    "t=30\n",
    "n_samples=10000\n",
    "X, y = make_classification(n_features=15,n_samples=n_samples,random_state=42,n_classes=k,n_informative=8)\n",
    "XD=KMeans(n_clusters=t).fit(X).labels_\n",
    "def checkarray(A,B):\n",
    "    bandera=0\n",
    "    for i in range(len(A)):\n",
    "        if np.abs(A[i]-B[i])>0.0001:\n",
    "            print(A[i],B[i])\n",
    "            bandera=1\n",
    "            print(\"error\")\n",
    "    if bandera==0:\n",
    "        print(\"ok\")\n",
    "\n",
    "def checkMatriz(A,B):\n",
    "    bandera=0\n",
    "    for i in range(len(A)):\n",
    "        for j in range(len(A[0])):\n",
    "            if A[i,j]!=B[i,j]:\n",
    "                bandera=1\n",
    "                print(\"errror\")\n",
    "    if bandera==0:\n",
    "        print(\"ok\")\n",
    "XD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26, 24, 30, ...,  4,  9, 40])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008965253829956055\n",
      "0.0019936561584472656\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "def compute_pHatOrginal(XD: np.ndarray, y: np.ndarray, K: int, T: int):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    XD : ndarray of shape (n_samples,)\n",
    "        Labels of profiles for each data point\n",
    "\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Labels\n",
    "\n",
    "    K : int\n",
    "        Number of classes\n",
    "\n",
    "    T : int\n",
    "        Number of profiles\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pHat : ndarray of shape(K, n_profiles)\n",
    "    \"\"\"\n",
    "    pHat = np.zeros((K, T))\n",
    "\n",
    "    for k in range(K):\n",
    "        Ik = np.where(y == k)[0]\n",
    "        mk = len(Ik)\n",
    "        for t in range(T):\n",
    "            pHat[k, t] = np.sum(XD[Ik] == t) / mk\n",
    "    return pHat\n",
    "\n",
    "def compute_pHatOpti(XD: np.ndarray, y: np.ndarray, K: int, T: int):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    XD : ndarray of shape (n_samples,)\n",
    "        Labels of profiles for each data point\n",
    "\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Labels\n",
    "\n",
    "    K : int\n",
    "        Number of classes\n",
    "\n",
    "    T : int\n",
    "        Number of profiles\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pHat : ndarray of shape(K, n_profiles)\n",
    "    \"\"\"\n",
    "    pHat = np.zeros((K, T))\n",
    "\n",
    "    for k in range(K):\n",
    "        Ik = np.where(y == k)[0]\n",
    "        mk = len(Ik)\n",
    "        \n",
    "        pHat[k] = np.bincount(XD[Ik], minlength=T)/mk\n",
    "    return pHat\n",
    "\n",
    "start=time.time()\n",
    "B=compute_pHatOrginal(XD,y,k,t)\n",
    "print(time.time()-start)\n",
    "start=time.time()\n",
    "A=compute_pHatOpti(XD,y,k,t)\n",
    "print(time.time()-start)\n",
    "checkMatriz(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8880321979522705\n",
      "0.004982709884643555\n",
      "0.0019893646240234375\n",
      "ok\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred=np.random.randint(0, k, size=n_samples)\n",
    "L=np.ones((k, k)) - np.eye(k)\n",
    "\n",
    "def compute_conditional_riskWenlong(y_true: np.ndarray, y_pred: np.ndarray, K: int, L: np.ndarray):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray of shape (n_samples,)\n",
    "        Real labels\n",
    "\n",
    "    y_pred : ndarray of shape (n_samples,)\n",
    "        Predicted labels\n",
    "\n",
    "    K : int\n",
    "        Number of classes\n",
    "\n",
    "    L : ndarray of shape (K, K)\n",
    "        Loss matrix, where K is the number of unique classes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    R : ndarray of shape (K,)\n",
    "        Conditional risk\n",
    "\n",
    "    confmat : ndarray of shape (K, K)\n",
    "        Confusion matrix\n",
    "    \"\"\"\n",
    "    # Identify all unique classes\n",
    "    unique_classes = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    class_to_index = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "\n",
    "    # Initialize the confusion matrix\n",
    "    confmat = np.zeros((K, K))\n",
    "\n",
    "    # Populate the confusion matrix\n",
    "    for true_class in unique_classes:\n",
    "        true_class_index = class_to_index[true_class]\n",
    "        Ik = np.where(y_true == true_class)[0]\n",
    "        pred_classes_indices = [class_to_index[pred] for pred in y_pred[Ik]]\n",
    "        for pred_index in pred_classes_indices:\n",
    "            confmat[true_class_index, pred_index] += 1\n",
    "\n",
    "    # Normalize the rows of the confusion matrix to get probabilities\n",
    "    row_sums = confmat.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1  # Prevent division by zero for classes not in y_true\n",
    "    confmat /= row_sums\n",
    "\n",
    "    # Calculate the conditional risk for each true category\n",
    "    R = np.dot(L, confmat.T).diagonal()\n",
    "\n",
    "    return R, confmat\n",
    "\n",
    "def compute_conditional_riskSalva(YR, Yhat, K, L): \n",
    "    '''\n",
    "    Function to compute the class-conditional risks.\n",
    "    Parameters\n",
    "    ----------\n",
    "    YR : DataFrame\n",
    "        Real labels.\n",
    "    Yhat : Array\n",
    "        Predicted labels.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss Function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    R : Array of floats\n",
    "        Conditional risks.\n",
    "    confmat : Matrix\n",
    "        Confusion matrix.\n",
    "    '''\n",
    "    Labels=[i for i in range(K)]\n",
    "    confmat=confusion_matrix(np.array(YR),np.array(Yhat),normalize='true',labels=Labels)\n",
    "    R=np.sum(np.multiply(L, confmat),axis=1)\n",
    "\n",
    "   # Is only the confuns \n",
    "    \n",
    "    return R, confmat\n",
    "\n",
    "def compute_conditional_riskCyprien(YR, Yhat, K, L): \n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    YR : DataFrame\n",
    "        Real labels.\n",
    "    Yhat : Array\n",
    "        Predicted labels.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss Function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    R : Array of floats\n",
    "        Conditional risks.\n",
    "    confmat : Matrix\n",
    "        Confusion matrix.\n",
    "    '''\n",
    "   \n",
    "    confmat = np.zeros((K, K))\n",
    "    R = np.zeros((1, K))\n",
    "    YR_liste= YR.values.tolist()\n",
    "    for k in range(0, K):\n",
    "        mk = YR_liste.count([k+1])\n",
    "        if mk > 0:\n",
    "            Ik = np.where(YR == (k + 1))\n",
    "            for l in range(0, K):\n",
    "                confmat[k,l] = sum(Yhat[Ik[0]]==l+1)/mk\n",
    "        R[0,k] = L[k, :].dot(confmat[k, :]) \n",
    "    \n",
    "    return R, confmat\n",
    "start=time.time()\n",
    "O1,O2=compute_conditional_riskCyprien(pd.DataFrame(y+1),y_pred+1,k,L)\n",
    "print(time.time()-start)\n",
    "start=time.time()\n",
    "W1,W2=compute_conditional_riskWenlong(y,y_pred,k,L)\n",
    "print(time.time()-start)\n",
    "start=time.time()\n",
    "S1,S2=compute_conditional_riskSalva(y,y_pred,k,L)\n",
    "print(time.time()-start)\n",
    "checkMatriz(S2,W2)\n",
    "checkarray(S1,W1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_piStarSalva(pHat, y_train, K, L, T, N, optionPlot, Box):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    pHat : Array of floats\n",
    "        Probability estimate of observing the features profile in each class.\n",
    "    y_train : Dataframe\n",
    "        Real labels of the training set.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss Function.\n",
    "    T : int\n",
    "        Number of discrete profiles.\n",
    "    N : int\n",
    "        Number of iterations in the projected subgradient algorithm.\n",
    "    optionPlot : int {0,1}\n",
    "        1 plots figure,   0: does not plot figure.\n",
    "    Box : Array\n",
    "        {'none', matrix} : Box-constraints on the priors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    piStar : Array of floats\n",
    "        Least favorable priors.\n",
    "    rStar : float\n",
    "        Global risks.\n",
    "    RStar : Array of float\n",
    "        Conditional risks.\n",
    "    V_iter : Array\n",
    "        Values of the V function at each iteration.\n",
    "    stockpi : Array\n",
    "        Values of pi at each iteration.\n",
    "\n",
    "    \"\"\"\n",
    "    # IF BOX-CONSTRAINT == NONE (PROJECTION ONTO THE SIMPLEX)\n",
    "    if Box is None:\n",
    "        pi = compute_pi(y_train, K).reshape(1, -1)\n",
    "        rStar = 0\n",
    "        piStar = pi\n",
    "        RStar = 0\n",
    "\n",
    "        V_iter = []\n",
    "        stockpi = np.zeros((K, N))\n",
    "\n",
    "        for n in range(1, N + 1):\n",
    "            # Compute subgradient R at point pi (see equation (21) in the paper)\n",
    "            lambd = np.dot(L, pi.T * pHat)\n",
    "            R = np.zeros((1, K))\n",
    "\n",
    "            mu_k = np.sum(L[:, np.argmin(lambd, axis=0)] * pHat, axis=1)\n",
    "            R[0,:] = mu_k\n",
    "            stockpi[:,n-1] = pi[0,:]\n",
    "            \n",
    "            r = compute_global_risk(R, pi)\n",
    "            V_iter.append(r)\n",
    "            if r > rStar:\n",
    "                rStar = r\n",
    "                piStar = pi\n",
    "                RStar = R\n",
    "                # Update pi for iteration n+1\n",
    "            gamma = 1 / n\n",
    "            eta = np.maximum(float(1), np.linalg.norm(R))\n",
    "            w = pi + (gamma / eta) * R\n",
    "            pi = proj_simplex_Condat(K, w)\n",
    "\n",
    "        # Check if pi_N == piStar\n",
    "        lambd = np.dot(L, pi.T * pHat)\n",
    "        R = np.zeros((1, K))\n",
    "        for k in range(0, K):\n",
    "            mu_k = 0\n",
    "            for t in range(0, T):\n",
    "                lbar = np.argmin(lambd[:, t])\n",
    "                mu_k = mu_k + L[k, lbar] * pHat[k, t]\n",
    "            R[0, k] = mu_k\n",
    "            stockpi[k, n - 1] = pi[0, k]\n",
    "        r = compute_global_risk(R, pi)\n",
    "        if r > rStar:\n",
    "            rStar = r\n",
    "            piStar = pi\n",
    "            RStar = R\n",
    "\n",
    "        if optionPlot == 1:\n",
    "            graph_convergence(V_iter)\n",
    "\n",
    "    # IF BOX-CONSTRAINT\n",
    "    if Box is not None:\n",
    "        pi = compute_pi(y_train, K).reshape(1, -1)\n",
    "        rStar = 0\n",
    "        piStar = pi\n",
    "        RStar = 0\n",
    "\n",
    "        V_iter = []\n",
    "        stockpi = np.zeros((K, N))\n",
    "\n",
    "        for n in range(1, N + 1):\n",
    "            # Compute subgradient R at point pi (see equation (21) in the paper)\n",
    "            lambd = np.dot(L, pi.T * pHat)\n",
    "            R = np.zeros((1, K))\n",
    "            for k in range(0, K):\n",
    "                mu_k = 0\n",
    "                for t in range(0, T):\n",
    "                    lbar = np.argmin(lambd[:, t])\n",
    "                    mu_k = mu_k + L[k, lbar] * pHat[k, t]\n",
    "                R[0, k] = mu_k\n",
    "                stockpi[k, n - 1] = pi[0, k]\n",
    "            r = compute_global_risk(R, pi)\n",
    "            V_iter.append(r)\n",
    "            if r > rStar:\n",
    "                rStar = r\n",
    "                piStar = pi\n",
    "                RStar = R\n",
    "                # Update pi for iteration n+1\n",
    "            gamma = 1 / n\n",
    "            eta = np.maximum(float(1), np.linalg.norm(R))\n",
    "            w = pi + (gamma / eta) * R\n",
    "            pi = proj_onto_U(w, Box, K)\n",
    "\n",
    "        # Check if pi_N == piStar\n",
    "        lambd = np.dot(L, pi.T * pHat)\n",
    "        R = np.zeros((1, K))\n",
    "        for k in range(0, K):\n",
    "            mu_k = 0\n",
    "            for t in range(0, T):\n",
    "                lbar = np.argmin(lambd[:, t])\n",
    "                mu_k = mu_k + L[k, lbar] * pHat[k, t]\n",
    "            R[0, k] = mu_k\n",
    "            stockpi[k, n - 1] = pi[0, k]\n",
    "        r = compute_global_risk(R, pi)\n",
    "        if r > rStar:\n",
    "            rStar = r\n",
    "            piStar = pi\n",
    "            RStar = R\n",
    "\n",
    "        if optionPlot == 1:\n",
    "            graph_convergence(V_iter)\n",
    "\n",
    "    return piStar, rStar, RStar, V_iter, stockpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_piStarWenlong(pHat, y_train, K, L, T, N, optionPlot, Box):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    pHat : Array of floats\n",
    "        Probability estimate of observing the features profile in each class.\n",
    "    y_train : Dataframe\n",
    "        Real labels of the training set.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss Function.\n",
    "    T : int\n",
    "        Number of discrete profiles.\n",
    "    N : int\n",
    "        Number of iterations in the projected subgradient algorithm.\n",
    "    optionPlot : int {0,1}\n",
    "        1 plots figure,   0: does not plot figure.\n",
    "    Box : Array\n",
    "        {'none', matrix} : Box-constraints on the priors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    piStar : Array of floats\n",
    "        Least favorable priors.\n",
    "    rStar : float\n",
    "        Global risks.\n",
    "    RStar : Array of float\n",
    "        Conditional risks.\n",
    "    V_iter : Array\n",
    "        Values of the V function at each iteration.\n",
    "    stockpi : Array\n",
    "        Values of pi at each iteration.\n",
    "\n",
    "    \"\"\"\n",
    "    # IF BOX-CONSTRAINT == NONE (PROJECTION ONTO THE SIMPLEX)\n",
    "    if Box is None:\n",
    "        pi = compute_pi(y_train, K).reshape(1, -1)\n",
    "        rStar = 0\n",
    "        piStar = pi\n",
    "        RStar = 0\n",
    "\n",
    "        V_iter = []\n",
    "        stockpi = np.zeros((K, N))\n",
    "\n",
    "        for n in range(1, N + 1):\n",
    "            # Compute subgradient R at point pi (see equation (21) in the paper)\n",
    "            lambd = np.dot(L, pi.T * pHat)\n",
    "            R = np.zeros((1, K))\n",
    "            for k in range(0, K):\n",
    "                mu_k = 0\n",
    "                for t in range(0, T):\n",
    "                    lbar = np.argmin(lambd[:, t])\n",
    "                    mu_k = mu_k + L[k, lbar] * pHat[k, t]\n",
    "                R[0, k] = mu_k\n",
    "                stockpi[k, n - 1] = pi[0, k]\n",
    "            r = compute_global_risk(R, pi)\n",
    "            V_iter.append(r)\n",
    "            if r > rStar:\n",
    "                rStar = r\n",
    "                piStar = pi\n",
    "                RStar = R\n",
    "                # Update pi for iteration n+1\n",
    "            gamma = 1 / n\n",
    "            eta = np.maximum(float(1), np.linalg.norm(R))\n",
    "            w = pi + (gamma / eta) * R\n",
    "            pi = proj_simplex_Condat(K, w)\n",
    "\n",
    "        # Check if pi_N == piStar\n",
    "        lambd = np.dot(L, pi.T * pHat)\n",
    "        R = np.zeros((1, K))\n",
    "        for k in range(0, K):\n",
    "            mu_k = 0\n",
    "            for t in range(0, T):\n",
    "                lbar = np.argmin(lambd[:, t])\n",
    "                mu_k = mu_k + L[k, lbar] * pHat[k, t]\n",
    "            R[0, k] = mu_k\n",
    "            stockpi[k, n - 1] = pi[0, k]\n",
    "        r = compute_global_risk(R, pi)\n",
    "        if r > rStar:\n",
    "            rStar = r\n",
    "            piStar = pi\n",
    "            RStar = R\n",
    "\n",
    "        if optionPlot == 1:\n",
    "            graph_convergence(V_iter)\n",
    "\n",
    "    # IF BOX-CONSTRAINT\n",
    "    if Box is not None:\n",
    "        pi = compute_pi(y_train, K).reshape(1, -1)\n",
    "        rStar = 0\n",
    "        piStar = pi\n",
    "        RStar = 0\n",
    "\n",
    "        V_iter = []\n",
    "        stockpi = np.zeros((K, N))\n",
    "\n",
    "        for n in range(1, N + 1):\n",
    "            # Compute subgradient R at point pi (see equation (21) in the paper)\n",
    "            lambd = np.dot(L, pi.T * pHat)\n",
    "            R = np.zeros((1, K))\n",
    "            for k in range(0, K):\n",
    "                mu_k = 0\n",
    "                for t in range(0, T):\n",
    "                    lbar = np.argmin(lambd[:, t])\n",
    "                    mu_k = mu_k + L[k, lbar] * pHat[k, t]\n",
    "                R[0, k] = mu_k\n",
    "                stockpi[k, n - 1] = pi[0, k]\n",
    "            r = compute_global_risk(R, pi)\n",
    "            V_iter.append(r)\n",
    "            if r > rStar:\n",
    "                rStar = r\n",
    "                piStar = pi\n",
    "                RStar = R\n",
    "                # Update pi for iteration n+1\n",
    "            gamma = 1 / n\n",
    "            eta = np.maximum(float(1), np.linalg.norm(R))\n",
    "            w = pi + (gamma / eta) * R\n",
    "            pi = proj_onto_U(w, Box, K)\n",
    "\n",
    "        # Check if pi_N == piStar\n",
    "        lambd = np.dot(L, pi.T * pHat)\n",
    "        R = np.zeros((1, K))\n",
    "        for k in range(0, K):\n",
    "            mu_k = 0\n",
    "            for t in range(0, T):\n",
    "                lbar = np.argmin(lambd[:, t])\n",
    "                mu_k = mu_k + L[k, lbar] * pHat[k, t]\n",
    "            R[0, k] = mu_k\n",
    "            stockpi[k, n - 1] = pi[0, k]\n",
    "        r = compute_global_risk(R, pi)\n",
    "        if r > rStar:\n",
    "            rStar = r\n",
    "            piStar = pi\n",
    "            RStar = R\n",
    "\n",
    "        if optionPlot == 1:\n",
    "            graph_convergence(V_iter)\n",
    "\n",
    "    return piStar, rStar, RStar, V_iter, stockpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04975124, 0.09950249, 0.039801  , ..., 0.02487562, 0.00497512,\n",
       "        0.04975124],\n",
       "       [0.00502513, 0.15075377, 0.15075377, ..., 0.03517588, 0.04020101,\n",
       "        0.04020101],\n",
       "       [0.        , 0.005     , 0.005     , ..., 0.06      , 0.055     ,\n",
       "        0.01      ],\n",
       "       ...,\n",
       "       [0.01507538, 0.00502513, 0.08542714, ..., 0.01507538, 0.        ,\n",
       "        0.03517588],\n",
       "       [0.00502513, 0.0201005 , 0.01005025, ..., 0.0201005 , 0.01005025,\n",
       "        0.06030151],\n",
       "       [0.        , 0.        , 0.        , ..., 0.03535354, 0.08080808,\n",
       "        0.01010101]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phat=compute_pHat(XD, y, k, t)\n",
    "Phat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.8850407600402832\n"
     ]
    }
   ],
   "source": [
    "N=10000\n",
    "start=time.time()\n",
    "#W1,W2,W3,W4,W5=compute_piStarWenlong(Phat, y, k, L, t, N, 0, None)\n",
    "print(time.time()-start)\n",
    "start=time.time()\n",
    "pi,S2,S3,S4,S5=compute_piStarSalva(Phat, y, k, L, t, N, 0, None)\n",
    "print(time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "-1.1102230246251565e-16\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "checkarray(W1[0],S1[0])\n",
    "print(W2-S2)\n",
    "checkarray(W3[0],S3[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SoftminDMC_compute_CondRisksO(K, L, pHat, pi, lambd):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss function.\n",
    "    pHat : Array of floats\n",
    "        Probability estimate of observing the features profile.\n",
    "    pi : Array (a unique line) of floats\n",
    "        Priors.\n",
    "    lambd : Float\n",
    "        Positive Temperature parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    R_softminDMC : Array\n",
    "        Values of the class-conditional risks at the point pi.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    T = np.shape(pHat)[1]\n",
    "    F = np.zeros((K,T))\n",
    "    NUM_SIGM = np.zeros((K,T))\n",
    "    for t in range(0, T):\n",
    "        for l in range(0, K):\n",
    "            for j in range(0, K):\n",
    "                F[l,t] = F[l,t] + L[j,l] * pHat[j,t] * pi[0,j]\n",
    "            NUM_SIGM[l,t] = np.exp(-lambd * F[l,t])\n",
    "    DENUM_SIGM = np.sum(NUM_SIGM, axis=0)\n",
    "\n",
    "    R_softminDMC = np.zeros((1, K))\n",
    "    for k in range(0, K):\n",
    "        mu_k = 0\n",
    "        for t in range(0, T):\n",
    "            for l in range(0, K):\n",
    "                sigma_l = NUM_SIGM[l,t]/DENUM_SIGM[t]\n",
    "                mu_k = mu_k + L[k,l] * pHat[k,t] * sigma_l\n",
    "        R_softminDMC[0,k] = mu_k\n",
    "\n",
    "    return R_softminDMC[0]\n",
    "\n",
    "\n",
    "def SoftminDMC_compute_CondRisksS(K, L, pHat, pi, lambd):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss function.\n",
    "    pHat : Array of floats\n",
    "        Probability estimate of observing the features profile.\n",
    "    pi : Array (a unique line) of floats\n",
    "        Priors.\n",
    "    lambd : Float\n",
    "        Positive Temperature parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    R_softminDMC : Array\n",
    "        Values of the class-conditional risks at the point pi.\n",
    "    '''\n",
    "    \n",
    "    T = pHat.shape[1]\n",
    "    \n",
    "    # Compute F using vectorized operations\n",
    "    F = np.einsum('jl,jt,j->lt', L, pHat, pi[0])\n",
    "    \n",
    "    # Compute NUM_SIGM and DENUM_SIGM using vectorized operations\n",
    "    NUM_SIGM = np.exp(-lambd * F)\n",
    "    DENUM_SIGM = np.sum(NUM_SIGM, axis=0)\n",
    "    \n",
    "    # Compute sigma_l\n",
    "    sigma = NUM_SIGM / DENUM_SIGM\n",
    "    \n",
    "    # Compute mu_k and R_softminDMC using vectorized operations\n",
    "    R_softminDMC = np.einsum('kl,kt,lt->k', L, pHat, sigma)\n",
    "    \n",
    "    return R_softminDMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09667825698852539\n",
      "0.0009937286376953125\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso:\n",
    "\n",
    "start=time.time()\n",
    "resultado1 = SoftminDMC_compute_CondRisksO(k, L, Phat, pi, 2)\n",
    "print(time.time()-start)\n",
    "start=time.time()\n",
    "resultado2 = SoftminDMC_compute_CondRisksS(k, L, Phat, pi, 2)\n",
    "print(time.time()-start)\n",
    "\n",
    "#print(resultado1)\n",
    "#print(resultado2)\n",
    "checkarray(resultado1,resultado2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
